# -*- coding: utf-8 -*-
"""bib_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TX41AgJExIVF1XPrILaP74Ang9CjeAJv

# import
"""

import numpy as np
import pandas as pd
from tqdm import tqdm
import os, cv2, glob

#load iamges
from skimage.io import imread
from skimage.transform import resize

#split data
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator

#graph
import matplotlib.pyplot as plt
import seaborn as sns

"""## create dataSet"""

def create_dataSet(_path_data, _csv_data, _categories):
  """
  --> create data set in format csv
  :param: _path_data: path the data
  :param: _csv_data: path with file name .csv
  :param: _categories: the classes the data
  :return: DataFrame with file path
  """
  data=pd.DataFrame(columns= ['file', 'labels'])

  c=0
  #cat_names = os.listdir(_path_data)
  for j in tqdm(_categories):
      pathfile = _path_data+'/'+j
      filenames = os.listdir(pathfile)
      for i in filenames:
        #print(_path_data+'/'+j+'/'+i)
        data.loc[c] = [str(_path_data+'/'+j+'/'+i), j]
        c=c+1
  #print(c)
  data.to_csv(_csv_data, index = False, header=True)
  data_csv = pd.read_csv(_csv_data)
  print('\n path do csv_data: ',_csv_data)
  print(data_csv.groupby('labels').count())
  return data

def create_unlabelSet(path_unlabels):
  """
  --> create dataset unlabel
  :param: path_unlabels: path the dataSet unlabel
  :return: DataFrame with file path of images unlabel
  """
  unlabelsBD=pd.DataFrame(columns= ['file'])
  c=0
  filenames = os.listdir(path_unlabels)
  for i in filenames:
    #print(path_unlabels+'/'+i)
    unlabelsBD.loc[c] = [str(path_unlabels+'/'+i)]
    c=c+1
  print(c)
  return unlabelsBD

"""## create folders"""

def create_folders(_save_dir, flag=1):
  """
  -->create folders
  :param: _save_dir: path the folder
  :param: flag: rewrite the folder, (1 for not and display error: 'the folder already exists)
  """
  if os.path.isdir(_save_dir):
    if flag:
      raise FileNotFoundError("folders test already exists: ", _save_dir)
    else:
      print('folders test already exists: ', _save_dir)
  else:
      os.mkdir(_save_dir)
      print('create folders test: ', _save_dir)

"""## quantize dataSet"""

def quantizar_dataSet(path_data):
  """
  --> qauntize dataSet
  :param: path_data: path the dataSet
  :print: classes and the amount of images
  """
  print(path_data)

  data=pd.DataFrame(columns= ['file', 'labels'])

  c=0
  cat_names = os.listdir(path_data)
  for j in tqdm(cat_names):
      pathfile = path_data+'/'+j
      filenames = os.listdir(pathfile)
      for i in filenames:
        #print(path_data+'/'+j+'/'+i)
        data.loc[c] = [str(path_data+'/'+j+'/'+i), j]
        c=c+1
  print(data.groupby('labels').count())

"""## splitData"""

def splitData(data_csv, path_save, name_base):
  """
  -->Split dataSet into training and testing data
  :param: data_csv: dataSet in csv format
  :param: path_save: path to save training and testing data
  :param: name_base: name to save the data
  """

  prod_csv, test_csv = train_test_split(data_csv, test_size=0.2, shuffle=True)
  train_csv, val_csv = train_test_split(prod_csv, test_size=0.2, shuffle=True)
  #print('Train ',train_csv.shape)
  #print('Test ',test_csv.shape)

  #Salvar split data
  _csv_train=path_save+'/'+name_base+'_trainSet.csv'
  _csv_test=path_save+'/'+name_base+'_testSet.csv'
  _csv_val=path_save+'/'+name_base+'_valSet.csv'
  train_csv.to_csv(_csv_train, index = False, header=True)
  test_csv.to_csv(_csv_test, index = False, header=True)
  val_csv.to_csv(_csv_val, index = False, header=True)

  training_data = pd.read_csv(_csv_train)
  print('\n Train split')
  print(training_data.groupby('labels').count())
  test_data = pd.read_csv(_csv_test)
  print('\n Test split')
  print(test_data.groupby('labels').count())

  val_data = pd.read_csv(_csv_val)
  print('\n Val split')
  print(val_data.groupby('labels').count())

  return training_data,val_data,test_data

"""## kfold"""

def kfold_split(data_csv, path_save, _k, name_base):
  """
  -->Split dataSet kfolds into training, validating and testing data
  :param: data_csv: dataSet in csv format
  :param: path_save: path to save training, valindating and testing data
  :param: _k: amounts of folders
  :param: name_base: name to save the data
  """

  Y = data_csv[['labels']]
  n=len(Y)
  print('Total de dados ',n)
  kfold = StratifiedKFold(n_splits=int(_k), random_state=7, shuffle=True)

  # 1- divisÃ£o train, test
  k=1
  for production_index, test_index in kfold.split(np.zeros(n),Y):
    prod = data_csv.iloc[production_index]
    test = data_csv.iloc[test_index]
    #train split
    train_index, val_index = train_test_split(production_index, test_size=0.2)
    training_data = data_csv.iloc[train_index]
    val_data = data_csv.iloc[val_index]

    print('total_train_index ',len(train_index))
    print('total_test_index ',len(test_index))
    print('total_valid_index ',len(val_index))
    print(k)

    #Salvar split data
    _csv_train=path_save+'/'+name_base+'_trainSet_k'+str(k)+'.csv'
    _csv_val=path_save+'/'+name_base+'_valSet_k'+str(k)+'.csv'
    _csv_test=path_save+'/'+name_base+'_testSet_k'+str(k)+'.csv'

    training_data.to_csv(_csv_train, index = False, header=True)
    val_data.to_csv(_csv_val, index = False, header=True)
    test.to_csv(_csv_test, index = False, header=True)

    train = pd.read_csv(_csv_train)
    #print(train)
    print(train.groupby('labels').count())
    k+=1

"""## load images"""

def load_img(_csv_data, input_shape):
  """
  -->load images
  :param: _csv_data: path with file name .csv
  :param: input_shape: shape examples (224,224,3)
  """
  x=[]
  fnames,labels = _csv_data['file'],_csv_data['labels']
  for f in tqdm(fnames):
        img=imread(f)
        x.append(resize(img,output_shape=input_shape))
  return np.array(x),labels.values

def load_img_cat(data_path, classes_names, image_shape = (224, 224)):
  """
  -->load images by classes, the subfolders are the classes
  :param: data_path: path dataSet
  :param: classes_names: dataSet classes
  :param: image_shape: resize image for shape, examples (224,224)
  :return: X are images matrix, Y are classes and c are path the images
  :comments: images need to be normalized
  """
  X = []
  Y = []
  c = []

  i = 0

  for categorie in tqdm(classes_names):
      p = f'{data_path}/{categorie}'
      for img_name in os.listdir(p):
          x = cv2.imread(f'{p}/{img_name}')
          xf = cv2.resize(x, image_shape)
          X.append(xf)
          Y.append(i)
          c.append(p)
      i += 1

  X = np.array(X)
  Y = np.array(Y)
  c = np.array(c)

  print(classes_names)

  return X, Y, c

def load_img_un(data_path, input_shape):
  """
  --> load images in dataSet unlabel
  :param: data_path: path the dataSet
  :param: image_shape: resize image for shape, examples (224,224)
  :return: array of the normalized images
  """
  x=[]
  for f in tqdm(data_path['file']):
        img=imread(f)
        img = img/255
        x.append(resize(img,output_shape=input_shape))
  return np.array(x)

def load_img_folder_type(PATH_IN, tipo='png'):
  """
  --> load images in dataSet unlabel
  :param: PATH_IN: path the dataSet
  :param: tipo: specifique format image, standar 'png'
  :return: array of the images
  """
  images = []
  query=PATH_IN+'/*.'+tipo
  print(query)

  images_path = glob.glob(query)
  for img_path in images_path:
      img = imread(img_path)
      images.append(img)
  images=np.array(images)
  print(len(images))
  return images

def normalizatioin(arr_img):
  """
  --> normaliza array the images
  :param: arr_img: array the images
  :return: arr_norm: array the normalized images
  """
  arr_norm=[]
  for img in arr_img:
    img=img/255
    arr_norm.append(img)
  arr_norm=np.array(arr_norm)
  return arr_norm

"""## graph image for category"""

def graph_img_cat(data_dir, save_dir=''):
  """
  --> Graph image by category
  :param: data_dir: folder the dataSet
  :save_dir: folder for graphic saver
  :return: Graph of the number of images by category
  """
  category_names = sorted(os.listdir(data_dir))
  nb_categories = len(category_names)
  img_pr_cat = []
  for category in category_names:
      folder = data_dir + '/' + category
      img_pr_cat.append(len(os.listdir(folder)))

  plt.figure(figsize=(15,10))
  sns.barplot(y=category_names, x=img_pr_cat).set_title("Number of training images per category:")
  
  if not (save_dir == ''):
      plt.savefig(save_dir + '/img_cat.jpg')

"""## display"""

def display(images, figsize=(15, 15)):
  """
  --> show images, limts 36 size (6x6)
  :param: images: array images
  :return: plot images (6x6)
  """
  cont=len(images)
  print('images size ', cont)
  rz=cont//5
  if cont%5>=0:
    rz=rz+1
  plt.figure(1, figsize)
  for i in range(cont):
      plt.subplot(rz, 5, i+1)
      plt.imshow(images[i])
      plt.title(i)
      plt.axis('off')
  plt.show()

"""# Voting Majoritary functions"""

def gabarito(models, X_test, y_test):
  """
  --> 'receives the model with the images and
    labels to make the prediction and returns
    the max votes and the confidence of each vote
  :param: models: the models to predict
  :param: X_test: the array imagens
  :param: y_test: labels the images
  :return: max(indices the softmax), conf(softmax probability)
  """
  max=pd.DataFrame(y_test, columns=['imgs'])
  conf=pd.DataFrame(y_test, columns=['imgs'])
  preds=[]
  c=0
  for i in models:
    preds=i.predict(X_test)
    predMax=np.argmax(preds, axis=1)
    con=[]
    #print(predMax)
    for j in preds:
      index=np.argmax(j)
      confi=j[index]
      con.append(confi)
    con = np.array(con)
    #print(con)
    col='rd'+str(c)
    max.insert(c, column=col, value=predMax)
    col='cf'+str(c)
    conf.insert(c, column=col, value=con)
    c=c+1
  return max, conf

def findMajority(arr, n):
  """
  --> calculate the majority vote
  :param: arr: array the voting
  :param: n: array size
  :return: returns the winner, if there is no winner, returns -1
  """
  maxCount = 0
  index = -1  # sentinels
  for i in range(n):
    count = 1
    # here we compare the element in
    # ith position with i+1th position
    for j in range(i+1, n):
      if(arr[i] == arr[j]):
        count += 1

    # update maxCount if count of
    # current element is greater
    if(count > maxCount):
      maxCount = count
      index = i

  # if maxCount is greater than n/2
  # return the corresponding element
  if (maxCount > n//2):
    vencedor=arr[index]
    #print(vencedor)
  else:
    vencedor=-1
  return vencedor, index

def contagem_votos(max, conf, y_test):
  """
  --> vote count
  :param: max: array the voting
  :param: conf: softmax probability
  :param: y_test: real labels the images
  :return: DataFrame with image file name and classification
  """
  ven=[]
    
  for i in range(len(max)):
      votos=max.iloc[:,:-1].loc[i].values
      #print(votos)
      major, idx=findMajority(votos, len(votos))
      if major==-1: #Se o voto for empatado, verificar max da conf
        res=conf.iloc[:,:-1].loc[i].values 
        index=np.argmax(res) #procura o maior valor

        #print('res ',res, 'index ',index, 'votos ', votos[index], res[index])
        #print("-"*90)
        ven.append(votos[index])
        
      else:
        #print(major)
        ven.append(major)
    
  return ven

def nova_contagem_votos(max, conf, y_test, CATEGORIES):
  """
  --> vote count
  :param: max: array the voting
  :param: conf: softmax probability
  :param: y_test: real labels the images
  :return: DataFrame with image file name and classification
  """
  ven = pd.DataFrame(columns= ['labels', 'predict', 'classe', 'confianca', 'rede'])
  
  for i in range(len(max)):
      votos=max.iloc[:,:-1].loc[i].values
      #print(votos)
      major, idx=findMajority(votos, len(votos))
      if major==-1: #Se o voto for empatado, verificar max da conf
        confs=conf.iloc[:,:-1].loc[i].values 
        index=np.argmax(confs) #procura o maior valor

        ven.loc[len(ven)]=[y_test[i], CATEGORIES[votos[index]], votos[index], confs[index], index]
        print(confs)
        print(votos)
        print('label ', y_test[i], 'votos ', votos[index], 'confianca ',  confs[index], 'indice ', index)
        
      else:
        confs=conf.iloc[:,:-1].loc[i].values 
        print(confs)
        print(votos)
        print('label ', y_test[i], 'major ', major,'confianca ', confs[idx], 'indice ',idx)
        
        ven.loc[len(ven)]=[y_test[i], CATEGORIES[major], major, confs[idx],idx]

  return ven

def count_vots_2(max, conf, y_test, CATEGORIES, save_dir, fold_var):
  """
  --> vote count
  :param: max: array the voting
  :param: conf: softmax probability
  :param: y_test: real labels the images
  :return: DataFrame with image file name and classification
  """
  ven = pd.DataFrame(columns= ['labels', 'predict', 'classe', 'confianca', 'rede', 'sit'])
  
  for i in range(len(max)):
      votos=max.iloc[:,:-1].loc[i].values
      #print(votos)
      #major, idx=findMajority(votos, len(votos))
      
      if votos[0]==votos[1]: #Se o voto forem iguais
        confs=conf.iloc[:,:-1].loc[i].values 
        index=np.argmax(confs) #procura o maior valor
        p=CATEGORIES[votos[index]]
        y=y_test[i]
        
        
        if y==p:
            ven.loc[len(ven)]= [y, p, votos[index], confs[index], index, 'C']
        else:
            ven.loc[len(ven)]=[y, p, votos[index], confs[index], index, 'E']
            
        
      else:
        confs=conf.iloc[:,:-1].loc[i].values 
        index=np.argmax(confs) #procura o maior valor
        p=CATEGORIES[votos[index]]
        y=y_test[i]
        
        if y==p:
            ven.loc[len(ven)]=[y, p, votos[index], confs[index], index, 'C']
        else:
            ven.loc[len(ven)]=[y, p, votos[index], confs[index], index, 'E']
        
  print('Size correct: ',len(ven))
  ven2 = ven.sort_values(by='labels')
  ven2.to_csv(save_dir+'/filterCorrect'+str(fold_var)+'.csv', index= True)
  return ven2

"""# Main"""

if __name__=="__main__":
  help(create_folders)
  help(create_dataSet)
  help(create_unlabelSet)
  help(load_img)
  help(quantizar_dataSet)
  help(graph_img_cat)
  help(load_img_folder_type)
  help(display)